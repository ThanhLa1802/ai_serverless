# ğŸ“„ Serverless RAG Chatbot on AWS

A **production-ready Serverless Retrieval-Augmented Generation (RAG) system** built on AWS Lambda, API Gateway, Pinecone, and OpenAI. The system allows users to upload documents (PDF) and ask natural language questions via a ChatGPT-like web UI.

---<img width="1310" height="580" alt="rag" src="https://github.com/user-attachments/assets/5e465dd7-e0fb-449e-b372-9a6e7ed2eee6" />


## ğŸš€ Key Features

- ğŸ” **Semantic Search (RAG)** using vector embeddings
- ğŸ“š **PDF ingestion & chunking**
- ğŸ§  **LLM-based answer generation** (OpenAI)
- âš¡ **Serverless architecture** (AWS Lambda + API Gateway)
- ğŸ³ **Container-based Lambda (ECR)**
- ğŸŒ **ChatGPT-like Web UI** (pure HTML/CSS/JS)
- ğŸ” Secrets managed via **GitHub Secrets**
- ğŸ“¦ Infrastructure as Code with **Terraform**
- ğŸ”„ CI/CD with **GitHub Actions**

---

## ğŸ—ï¸ High-Level Architecture

```
User (Browser)
   â†“
index.html (Chat UI)
   â†“  POST /ask
AWS API Gateway
   â†“
AWS Lambda (Query Handler)
   â”œâ”€ Retrieval â†’ Pinecone Vector DB
   â””â”€ Generation â†’ OpenAI Chat Model
   â†“
Answer returned to UI
```

A separate **Ingestion Lambda** is used to process and embed documents.

---

## ğŸ“‚ Project Structure

```
SERVERLESS_RAG_PROJECT
â”œâ”€â”€ .github/
â”‚   â””â”€â”€ workflows/
â”‚       â””â”€â”€ deploy.yml          # CI/CD pipeline
â”œâ”€â”€ infra/                      # Terraform IaC
â”‚   â”œâ”€â”€ main.tf
â”‚   â”œâ”€â”€ variables.tf
â”‚   â””â”€â”€ outputs.tf
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ common/
â”‚   â”‚   â””â”€â”€ logger.py
â”‚   â”œâ”€â”€ ingestion/
â”‚   â”‚   â”œâ”€â”€ handler.py          # Document ingestion Lambda
â”‚   â”‚   â””â”€â”€ service.py
â”‚   â””â”€â”€ retrieval/
â”‚       â”œâ”€â”€ handler.py          # Query Lambda
â”‚       â”œâ”€â”€ search.py
â”‚       â””â”€â”€ generator.py
â”œâ”€â”€ index.html                  # ChatGPT-like frontend
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

---

## ğŸ§  RAG Flow Explained

### 1ï¸âƒ£ Ingestion Phase

- Upload PDF documents
- Extract text per page
- Split text into chunks
- Generate embeddings using **OpenAI Embeddings**
- Store vectors in **Pinecone**

### 2ï¸âƒ£ Query Phase

- User submits a question
- Convert question to embedding
- Perform similarity search in Pinecone
- Retrieve top-K relevant chunks
- Inject context into prompt
- Generate final answer via OpenAI Chat Model

---

## ğŸ› ï¸ Technology Stack

| Layer | Technology |
|-----|-----------|
| Frontend | HTML, CSS, Vanilla JS |
| API | AWS API Gateway |
| Compute | AWS Lambda (Container Image) |
| Container | Docker + Amazon ECR |
| Vector DB | Pinecone |
| Embeddings | OpenAI `text-embedding-3-small` |
| LLM | OpenAI Chat Models |
| IaC | Terraform |
| CI/CD | GitHub Actions |

---

## ğŸ” Environment Variables

Configured via **GitHub Secrets** and injected by Terraform:

```env
OPENAI_API_KEY=
PINECONE_API_KEY=
PINECONE_INDEX_NAME=
PINECONE_NAMESPACE=default
```

> âš ï¸ Changing env vars **does NOT require rebuilding Docker images** â€” only a redeploy.

---

## ğŸ”„ CI/CD Pipeline (GitHub Actions)

Triggered on push to `master`:

1. Checkout code
2. Build Docker image
3. Push image to Amazon ECR
4. Run Terraform
5. Update Lambda functions

Image tagging strategy:
- `latest`
- `${GITHUB_SHA}` (immutable)

---

## ğŸ³ Docker & Lambda

- Base image: `public.ecr.aws/lambda/python:3.12`
- Optimized for fast cold start
- No heavy ML libraries (no `torch`, no `sentence-transformers`)

Lambda handlers:

```python
src.ingestion.handler.handler
src.retrieval.handler.handler
```

---

## ğŸŒ Frontend Usage

Open `index.html` in a browser and ask questions:

```json
POST /ask
{
  "query": "What is the main content of ML.pdf?"
}
```

Features:
- ChatGPT-style UI
- User / Bot message alignment
- Loading indicator
- CORS enabled

---

## ğŸ’° Cost Considerations

- ğŸ’µ Pay-as-you-go (OpenAI)
- Pinecone: free / starter tier suitable for small docs
- AWS Lambda: extremely low cost for light usage

Example:
- 1 PDF (~6 pages): **< $0.01** embedding cost
- Typical query: **fractions of a cent**

---

## âœ… Production Best Practices Applied

- No secrets in Docker images
- Immutable image tags
- Environment-based configuration
- Stateless Lambdas
- Warm-start optimization

---

## ğŸš§ Future Improvements

- ğŸ” Authentication (Cognito / JWT)
- ğŸ“ Source citation in answers
- ğŸ”„ Streaming responses
- ğŸ“Š Observability & metrics
- ğŸŒ Multi-language support

---

## ğŸ‘¤ Author

Built by **Thanh** â€“ Backend Engineer

> Focused on AWS, Serverless, and AI-powered systems.

---

## â­ Summary

This project demonstrates a **clean, scalable, and production-ready RAG architecture** using modern cloud-native and AI technologies.

If you are learning **Serverless + GenAI**, this is a solid real-world reference.

